{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook using kfp.create_component_from_func function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05M92t4L_U51"
   },
   "source": [
    "### Telco Churn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05M92t4L_U51"
   },
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "import kfp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zfou2iW5_U6C"
   },
   "outputs": [],
   "source": [
    "## Read Data\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(file_name: str) -> 'pd.DataFrame': \n",
    "        \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #This line may cause problems as file is on the system and not inside container\n",
    "    #Importing directly from Github Raw Content\n",
    "    \n",
    "    df_churn = pd.read_csv(file_name, error_bad_lines=False)\n",
    "    col1 = len(df_churn.columns)\n",
    "    \n",
    "    empty_cols=['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "           'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "    \n",
    "    for i in empty_cols:\n",
    "        df_churn[i]=df_churn[i].replace(\" \",np.nan)\n",
    "\n",
    "    df_churn.drop(['customerID','cluster number'], axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    #df_churn.to_string()\n",
    "    return df_churn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohRWvYOQ_U6H"
   },
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.create_component_from_func(func = read_data, \n",
    "                                                          output_component_file = './read-data-func.yaml',\n",
    "                                                          packages_to_install = ['numpy','pandas'])\n",
    "\n",
    "read_data_task = kfp_read_data(file_name = 'https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0amunpu_U6N"
   },
   "outputs": [],
   "source": [
    "## One-Hot-Encode\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(input_df: 'pd.DataFrame') -> 'pd.DataFrame': \n",
    "\n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_churn = input_df #pd.read_csv(file_name)  \n",
    "    \n",
    "    binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']\n",
    "\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    #Encoding column 'gender'\n",
    "    df_churn['gender'] = df_churn['gender'].replace({\"Male\":1,\"Female\":0})\n",
    "\n",
    "\n",
    "    category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',\n",
    "                   'OnlineBackup','DeviceProtection',\n",
    "                   'TechSupport','StreamingTV','StreamingMovies','Contract']\n",
    "\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "    \n",
    "    df_churn['Churn'] = df_churn['Churn'].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    \n",
    "    #saving files may need a PV allocation to container\n",
    "    #output of files as Named tuple may cause problems    \n",
    "    \n",
    "    return df_churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3HCxWyE_U6S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:189: UserWarning: There are no registered serializers from type \"OrderedDict\" to type \"pd.DataFrame\", so the value will be serializers as string \"OrderedDict([('Output', TaskOutputArgument(task_output=TaskOutputReference(output_name='Output', task_id=None, task=TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name='Read data', description=None, metadata=None, inputs=[InputSpec(name='file_name', type='String', description=None, default=None, optional=False)], outputs=[OutputSpec(name='Output', type='pd.DataFrame', description=None)], implementation=ContainerImplementation(container=ContainerSpec(image='tensorflow/tensorflow:1.13.2-py3', command=['sh', '-c', '(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'numpy\\' \\'pandas\\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'numpy\\' \\'pandas\\' --user) && \"$0\" \"$@\"', 'python3', '-u', '-c', 'def read_data(file_name )  : \\n\\n    ## Import Required Libraries\\n    import pandas as pd\\n    import numpy as np\\n\\n    #This line may cause problems as file is on the system and not inside container\\n    #Importing directly from Github Raw Content\\n\\n    df_churn = pd.read_csv(file_name, error_bad_lines=False)\\n    col1 = len(df_churn.columns)\\n\\n    empty_cols=[\\'customerID\\', \\'gender\\', \\'SeniorCitizen\\', \\'Partner\\', \\'Dependents\\',\\n           \\'tenure\\', \\'PhoneService\\', \\'MultipleLines\\', \\'InternetService\\',\\n           \\'OnlineSecurity\\', \\'OnlineBackup\\', \\'DeviceProtection\\',\\'TechSupport\\',\\n           \\'StreamingTV\\', \\'StreamingMovies\\', \\'Contract\\', \\'PaperlessBilling\\',\\n           \\'PaymentMethod\\', \\'MonthlyCharges\\', \\'TotalCharges\\', \\'Churn\\']\\n\\n    for i in empty_cols:\\n        df_churn[i]=df_churn[i].replace(\" \",np.nan)\\n\\n    df_churn.drop([\\'customerID\\',\\'cluster number\\'], axis=1, inplace=True)\\n    df_churn = df_churn.dropna()\\n    #df_churn.to_string()\\n    return df_churn\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\\'Read data\\', description=\\'\\')\\n_parser.add_argument(\"--file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\"_output_paths\", [])\\n\\n_outputs = read_data(**_parsed_args)\\n\\nif not hasattr(_outputs, \\'__getitem__\\') or isinstance(_outputs, str):\\n    _outputs = [_outputs]\\n\\n_output_serializers = [\\n    str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, \\'w\\') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n'], args=['--file-name', InputValuePlaceholder(input_name='file_name'), '----output-paths', OutputPathPlaceholder(output_name='Output')], env=None, file_outputs=None)), version='google.com/cloud/pipelines/component/v1')), arguments={'file_name': 'https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv'}, is_enabled=None, execution_options=None), type='pd.DataFrame')))])\".\n",
      "  serialized_value),\n"
     ]
    }
   ],
   "source": [
    "kfp_one_hot_encode = kfp.components.create_component_from_func(func = one_hot_encode, \n",
    "                                                          output_component_file = './one-hot-encode-func.yaml',\n",
    "                                                          packages_to_install = ['numpy','pandas'])\n",
    "one_hot_encode_task = kfp_one_hot_encode(read_data_task.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHhL-J_X_U6Y"
   },
   "outputs": [],
   "source": [
    "## Random Forest Model\n",
    "from typing import NamedTuple\n",
    "def rf_model(input_df: 'pd.DataFrame', n_estimators: int = 100) -> NamedTuple('Outputs', [('Cf1', int), ('Cf2', int),\n",
    "                                                                                     ('Cf3', int), ('Cf4', int)]):\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    df_churn = input_df\n",
    "    n_est = n_estimators\n",
    "    \n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = df_churn.drop(['Churn'],axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "\n",
    "    sm = SMOTE(random_state=0)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth' : [2,4,5,6,7,8],\n",
    "        'criterion' :['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "\n",
    "    rfc=RandomForestClassifier(random_state=42,n_estimators=n_est)\n",
    "    gsv_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "    rfc.fit(X_train_res, y_train_res)\n",
    "\n",
    "    rfc_best=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 50, max_depth=8,\n",
    "                                    criterion='gini')\n",
    "\n",
    "    rfc_best.fit(X_train_res, y_train_res)\n",
    "    X_test_res, y_test_res = sm.fit_sample(X_test, y_test)\n",
    "    y_test_pred = rfc_best.predict(X_test_res)\n",
    "    rf_score = rfc_best.score(X_test_res, y_test_res)\n",
    "    conf = confusion_matrix(y_test_res, y_test_pred)\n",
    "    return (conf[0][0],conf[0][1],conf[1][0],conf[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HfsLs9l_U6i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:189: UserWarning: There are no registered serializers from type \"OrderedDict\" to type \"pd.DataFrame\", so the value will be serializers as string \"OrderedDict([('Output', TaskOutputArgument(task_output=TaskOutputReference(output_name='Output', task_id=None, task=TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name='One hot encode', description=None, metadata=None, inputs=[InputSpec(name='input_df', type='pd.DataFrame', description=None, default=None, optional=False)], outputs=[OutputSpec(name='Output', type='pd.DataFrame', description=None)], implementation=ContainerImplementation(container=ContainerSpec(image='tensorflow/tensorflow:1.13.2-py3', command=['sh', '-c', '(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'numpy\\' \\'pandas\\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'numpy\\' \\'pandas\\' --user) && \"$0\" \"$@\"', 'python3', '-u', '-c', 'def one_hot_encode(input_df )  : \\n\\n    ## Import Required Libraries\\n    import pandas as pd\\n    import numpy as np\\n\\n    df_churn = input_df #pd.read_csv(file_name)  \\n\\n    binary_cols = [\\'Partner\\',\\'Dependents\\',\\'PhoneService\\',\\'PaperlessBilling\\']\\n\\n    for i in binary_cols:\\n        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\\n\\n    #Encoding column \\'gender\\'\\n    df_churn[\\'gender\\'] = df_churn[\\'gender\\'].replace({\"Male\":1,\"Female\":0})\\n\\n    category_cols = [\\'PaymentMethod\\',\\'MultipleLines\\',\\'InternetService\\',\\'OnlineSecurity\\',\\n                   \\'OnlineBackup\\',\\'DeviceProtection\\',\\n                   \\'TechSupport\\',\\'StreamingTV\\',\\'StreamingMovies\\',\\'Contract\\']\\n\\n    for cc in category_cols:\\n        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\\n        dummies = dummies.add_prefix(\"{}#\".format(cc))\\n        df_churn.drop(cc, axis=1, inplace=True)\\n        df_churn = df_churn.join(dummies)\\n\\n    df_churn[\\'Churn\\'] = df_churn[\\'Churn\\'].replace({\"Yes\":1,\"No\":0})\\n\\n    #saving files may need a PV allocation to container\\n    #output of files as Named tuple may cause problems    \\n\\n    return df_churn \\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\\'One hot encode\\', description=\\'\\')\\n_parser.add_argument(\"--input-df\", dest=\"input_df\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\"_output_paths\", [])\\n\\n_outputs = one_hot_encode(**_parsed_args)\\n\\nif not hasattr(_outputs, \\'__getitem__\\') or isinstance(_outputs, str):\\n    _outputs = [_outputs]\\n\\n_output_serializers = [\\n    str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, \\'w\\') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n'], args=['--input-df', InputValuePlaceholder(input_name='input_df'), '----output-paths', OutputPathPlaceholder(output_name='Output')], env=None, file_outputs=None)), version='google.com/cloud/pipelines/component/v1')), arguments={'input_df': 'OrderedDict([(\\'Output\\', TaskOutputArgument(task_output=TaskOutputReference(output_name=\\'Output\\', task_id=None, task=TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name=\\'Read data\\', description=None, metadata=None, inputs=[InputSpec(name=\\'file_name\\', type=\\'String\\', description=None, default=None, optional=False)], outputs=[OutputSpec(name=\\'Output\\', type=\\'pd.DataFrame\\', description=None)], implementation=ContainerImplementation(container=ContainerSpec(image=\\'tensorflow/tensorflow:1.13.2-py3\\', command=[\\'sh\\', \\'-c\\', \\'(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\\\\\'numpy\\\\\\' \\\\\\'pandas\\\\\\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\\\\\'numpy\\\\\\' \\\\\\'pandas\\\\\\' --user) && \"$0\" \"$@\"\\', \\'python3\\', \\'-u\\', \\'-c\\', \\'def read_data(file_name )  : \\\\n\\\\n    ## Import Required Libraries\\\\n    import pandas as pd\\\\n    import numpy as np\\\\n\\\\n    #This line may cause problems as file is on the system and not inside container\\\\n    #Importing directly from Github Raw Content\\\\n\\\\n    df_churn = pd.read_csv(file_name, error_bad_lines=False)\\\\n    col1 = len(df_churn.columns)\\\\n\\\\n    empty_cols=[\\\\\\'customerID\\\\\\', \\\\\\'gender\\\\\\', \\\\\\'SeniorCitizen\\\\\\', \\\\\\'Partner\\\\\\', \\\\\\'Dependents\\\\\\',\\\\n           \\\\\\'tenure\\\\\\', \\\\\\'PhoneService\\\\\\', \\\\\\'MultipleLines\\\\\\', \\\\\\'InternetService\\\\\\',\\\\n           \\\\\\'OnlineSecurity\\\\\\', \\\\\\'OnlineBackup\\\\\\', \\\\\\'DeviceProtection\\\\\\',\\\\\\'TechSupport\\\\\\',\\\\n           \\\\\\'StreamingTV\\\\\\', \\\\\\'StreamingMovies\\\\\\', \\\\\\'Contract\\\\\\', \\\\\\'PaperlessBilling\\\\\\',\\\\n           \\\\\\'PaymentMethod\\\\\\', \\\\\\'MonthlyCharges\\\\\\', \\\\\\'TotalCharges\\\\\\', \\\\\\'Churn\\\\\\']\\\\n\\\\n    for i in empty_cols:\\\\n        df_churn[i]=df_churn[i].replace(\" \",np.nan)\\\\n\\\\n    df_churn.drop([\\\\\\'customerID\\\\\\',\\\\\\'cluster number\\\\\\'], axis=1, inplace=True)\\\\n    df_churn = df_churn.dropna()\\\\n    #df_churn.to_string()\\\\n    return df_churn\\\\n\\\\nimport argparse\\\\n_parser = argparse.ArgumentParser(prog=\\\\\\'Read data\\\\\\', description=\\\\\\'\\\\\\')\\\\n_parser.add_argument(\"--file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\\\\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\\\\n_parsed_args = vars(_parser.parse_args())\\\\n_output_files = _parsed_args.pop(\"_output_paths\", [])\\\\n\\\\n_outputs = read_data(**_parsed_args)\\\\n\\\\nif not hasattr(_outputs, \\\\\\'__getitem__\\\\\\') or isinstance(_outputs, str):\\\\n    _outputs = [_outputs]\\\\n\\\\n_output_serializers = [\\\\n    str,\\\\n\\\\n]\\\\n\\\\nimport os\\\\nfor idx, output_file in enumerate(_output_files):\\\\n    try:\\\\n        os.makedirs(os.path.dirname(output_file))\\\\n    except OSError:\\\\n        pass\\\\n    with open(output_file, \\\\\\'w\\\\\\') as f:\\\\n        f.write(_output_serializers[idx](_outputs[idx]))\\\\n\\'], args=[\\'--file-name\\', InputValuePlaceholder(input_name=\\'file_name\\'), \\'----output-paths\\', OutputPathPlaceholder(output_name=\\'Output\\')], env=None, file_outputs=None)), version=\\'google.com/cloud/pipelines/component/v1\\')), arguments={\\'file_name\\': \\'https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv\\'}, is_enabled=None, execution_options=None), type=\\'pd.DataFrame\\')))])'}, is_enabled=None, execution_options=None), type='pd.DataFrame')))])\".\n",
      "  serialized_value),\n"
     ]
    }
   ],
   "source": [
    "kfp_rf_model = kfp.components.create_component_from_func(func = rf_model, \n",
    "                                                          output_component_file = './rf-model-func.yaml',\n",
    "                                                          packages_to_install = ['scikit-learn==0.19.1','numpy','pandas','imbalanced-learn==0.6.2'])\n",
    "rf_model_task = kfp_rf_model(one_hot_encode_task.outputs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the components into pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PQNawQS_U6n"
   },
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(name='Merchant-Churn-Pipeline',description='A pipeline that processes and performs ML-Predictions using Random Forest Algorithm')\n",
    "def Merch_Churn(file_name = \"https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv\", \n",
    "                n_estimators = 100):\n",
    "    \n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    #Passing a task output reference as operation arguments\n",
    "    \n",
    "    read_data_task = kfp_read_data(file_name)    #Returns a dsl.ContainerOp class instance. \n",
    "    one_hot_encode_task = kfp_one_hot_encode(read_data_task.output) \n",
    "    rf_model_task = kfp_rf_model(one_hot_encode_task.output, n_estimators = 100)\n",
    "    \n",
    "\n",
    "#For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "#For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vXA3IA4Z_U6s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"100\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "pipeline_func = Merch_Churn\n",
    "pipeline_filename = pipeline_func.__name__+'.pipeline.tar.gz'\n",
    "\n",
    "import kfp.compiler as comp\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mc_final_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
