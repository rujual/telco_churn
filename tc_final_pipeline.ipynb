{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telco Churn Pipeline\n",
    "\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def read_data(file_name: InputPath('CSV')) -> 'pd.DataFrame':   \n",
    "    \n",
    "    #OutputPath('CSV'):\n",
    "        # -> NamedTuple('Outputs', [('Cols_drop', int),('Cols_retained', int)]):\n",
    "    \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #This line may cause problems as file is on the system and not inside container\n",
    "\n",
    "    df_churn = pd.read_csv(file_name)\n",
    "    col1 = len(df_churn.columns)\n",
    "    \n",
    "    empty_cols=['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "           'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "    \n",
    "    for i in empty_cols:\n",
    "        df_churn[i]=df_churn[i].replace(\" \",np.nan)\n",
    "\n",
    "    df_churn.drop('customerID','cluster number', axis=1, inplace=True)\n",
    "    df_churn = df_churn.dropna()\n",
    "    \n",
    "    col2 = len(df.columns)\n",
    "    #df_churn.to_csv('Cleaned_data.csv')\n",
    "    #out_path = \"./Cleaned_data.csv\"\n",
    "    \n",
    "    return df_churn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_read_data = kfp.components.create_component_from_func(func = read_data, \n",
    "                                                          output_component_file = './read-data-func.yaml',\n",
    "                                                          #base_image = 'fastgenomics/sklearn',\n",
    "                                                          packages_to_install = ['scikit-learn==0.19.1','numpy==1.17.2','pandas==0.25.1'])\n",
    "\n",
    "read_data_task = kfp_read_data(file_name = 'https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv')    #, out_file_name = 'Cleaned_data.csv')\n",
    "#,out_file_name = 'Cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-Hot-Encode\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.components import *\n",
    "\n",
    "def one_hot_encode(input_df: 'pd.DataFrame') -> 'pd.DataFrame': #file_name: InputPath('CSV')) -> OutputPath:\n",
    "                   \n",
    "#                    out_file1_name: str, \n",
    "#                    out_file2_name: str) -> NamedTuple('Outputs',\n",
    "#                                                       [('out_file1_name', OutputPath('CSV')),\n",
    "#                                                        ('out_file2_name', OutputPath('CSV'))]):\n",
    "    #out_file2_name: OutputPath('CSV')) -> None:\n",
    "    \n",
    "    ## Import Required Libraries\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    \n",
    "    df_churn = input_df #pd.read_csv(file_name)  \n",
    "    \n",
    "    binary_cols = ['Partner','Dependents','PhoneService','PaperlessBilling']\n",
    "\n",
    "    for i in binary_cols:\n",
    "        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    #Encoding column 'gender'\n",
    "    df_churn['gender'] = df_churn['gender'].replace({\"Male\":1,\"Female\":0})\n",
    "\n",
    "\n",
    "    category_cols = ['PaymentMethod','MultipleLines','InternetService','OnlineSecurity',\n",
    "                   'OnlineBackup','DeviceProtection',\n",
    "                   'TechSupport','StreamingTV','StreamingMovies','Contract']\n",
    "\n",
    "    for cc in category_cols:\n",
    "        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\n",
    "        dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "        df_churn.drop(cc, axis=1, inplace=True)\n",
    "        df_churn = df_churn.join(dummies)\n",
    "    \n",
    "    df_churn['Churn'] = df_churn['Churn'].replace({\"Yes\":1,\"No\":0})\n",
    "\n",
    "    \n",
    "    #saving files may need a PV allocation to container\n",
    "    #output of files as Named tuple may cause problems    \n",
    "    \n",
    "    #df_churn.to_csv('Oht_enc_data.csv')\n",
    "    #out_path = \"./Oht_enc_data.csv\"\n",
    "    return df_churn #out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:189: UserWarning: There are no registered serializers from type \"OrderedDict\" to type \"pd.DataFrame\", so the value will be serializers as string \"OrderedDict([('Output', TaskOutputArgument(task_output=TaskOutputReference(output_name='Output', task_id=None, task=TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name='Read data', description=None, metadata=None, inputs=[InputSpec(name='file_name', type='CSV', description=None, default=None, optional=False)], outputs=[OutputSpec(name='Output', type='pd.DataFrame', description=None)], implementation=ContainerImplementation(container=ContainerSpec(image='tensorflow/tensorflow:1.13.2-py3', command=['sh', '-c', '(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'scikit-learn==0.19.1\\' \\'numpy==1.17.2\\' \\'pandas==0.25.1\\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'scikit-learn==0.19.1\\' \\'numpy==1.17.2\\' \\'pandas==0.25.1\\' --user) && \"$0\" \"$@\"', 'python3', '-u', '-c', 'def read_data(file_name )  :   \\n\\n    #OutputPath(\\'CSV\\'):\\n        # -> NamedTuple(\\'Outputs\\', [(\\'Cols_drop\\', int),(\\'Cols_retained\\', int)]):\\n\\n    ## Import Required Libraries\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\n    import numpy as np\\n    import sklearn\\n\\n    #This line may cause problems as file is on the system and not inside container\\n\\n    df_churn = pd.read_csv(file_name)\\n    col1 = len(df_churn.columns)\\n    df_churn = df_churn.drop(columns=[])\\n\\n    empty_cols=[\\'customerID\\', \\'gender\\', \\'SeniorCitizen\\', \\'Partner\\', \\'Dependents\\',\\n           \\'tenure\\', \\'PhoneService\\', \\'MultipleLines\\', \\'InternetService\\',\\n           \\'OnlineSecurity\\', \\'OnlineBackup\\', \\'DeviceProtection\\',\\'TechSupport\\',\\n           \\'StreamingTV\\', \\'StreamingMovies\\', \\'Contract\\', \\'PaperlessBilling\\',\\n           \\'PaymentMethod\\', \\'MonthlyCharges\\', \\'TotalCharges\\', \\'Churn\\']\\n\\n    for i in empty_cols:\\n        df_churn[i]=df_churn[i].replace(\" \",np.nan)\\n\\n    df_churn.drop(\\'customerID\\',\\'cluster number\\', axis=1, inplace=True)\\n    df_churn = df_churn.dropna()\\n\\n    col2 = len(df.columns)\\n    #df_churn.to_csv(\\'Cleaned_data.csv\\')\\n    #out_path = \"./Cleaned_data.csv\"\\n\\n    return df_churn\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\\'Read data\\', description=\\'\\')\\n_parser.add_argument(\"--file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\"_output_paths\", [])\\n\\n_outputs = read_data(**_parsed_args)\\n\\nif not hasattr(_outputs, \\'__getitem__\\') or isinstance(_outputs, str):\\n    _outputs = [_outputs]\\n\\n_output_serializers = [\\n    str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, \\'w\\') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n'], args=['--file-name', InputPathPlaceholder(input_name='file_name'), '----output-paths', OutputPathPlaceholder(output_name='Output')], env=None, file_outputs=None)), version='google.com/cloud/pipelines/component/v1')), arguments={'file_name': 'https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv'}, is_enabled=None, execution_options=None), type='pd.DataFrame')))])\".\n",
      "  serialized_value),\n"
     ]
    }
   ],
   "source": [
    "kfp_one_hot_encode = kfp.components.create_component_from_func(func = one_hot_encode, \n",
    "                                                          output_component_file = './one-hot-encode-func.yaml',\n",
    "                                                          #base_image = 'fastgenomics/sklearn',\n",
    "                                                          packages_to_install = ['scikit-learn==0.19.1','numpy==1.17.2','pandas==0.25.1'])\n",
    "one_hot_encode_task = kfp_one_hot_encode(read_data_task.outputs) #'Oht_enc_data.csv')  #,'One_Hot_encoded_data.csv','Churn_flags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Model\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "def rf_model(input_df: 'pd.DataFrame', n_estimators: int = 100) -> NamedTuple('Outputs', [('Cf1', int), ('Cf2', int),\n",
    "                                                                                     ('Cf3', int), ('Cf4', int)]):\n",
    "#file_name: InputPath('CSV'), n_estimators: int) \n",
    "#ip_file1: InputPath('CSV'), ip_file2: InputPath('CSV'), modelopfile: OutputPath('joblib'))-> None:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import joblib\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    df_churn = input_df #pd.read_csv(file_name)\n",
    "    n_est = n_estimators\n",
    "    y1 = df_churn['Churn']\n",
    "    X1 = dfc_churn.drop(['churn_flag'],axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, random_state=0)\n",
    "    \n",
    "    sm = SMOTE(random_state=0)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth' : [2,4,5,6,7,8],\n",
    "        'criterion' :['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "\n",
    "    rfc=RandomForestClassifier(random_state=42,n_estimators=n_est)\n",
    "    gsv_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "    rfc.fit(X_train_res, y_train_res)\n",
    "\n",
    "\n",
    "    #rfc_best = gsv_rfc.best_estimator_\n",
    "    rfc_best=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 50, max_depth=8,\n",
    "                                    criterion='gini')\n",
    "\n",
    "    rfc_best.fit(X_train_res, y_train_res)\n",
    "    X_test_res, y_test_res = sm.fit_sample(X_test, y_test)\n",
    "    y_test_pred = rfc_best.predict(X_test_res, y_test_res)\n",
    "    rf_score = rfc_best.score(X_test_res, y_test_res)\n",
    "    conf = confusion_matrix(y_test_res, y_test_pred)\n",
    "\n",
    "    return (conf[0][0],conf[0][1],conf[1][0],conf[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #dump the trained model in pickle file\n",
    "    #joblib.dump(rfc_best, modelopfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:189: UserWarning: There are no registered serializers from type \"OrderedDict\" to type \"pd.DataFrame\", so the value will be serializers as string \"OrderedDict([('Output', TaskOutputArgument(task_output=TaskOutputReference(output_name='Output', task_id=None, task=TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name='One hot encode', description=None, metadata=None, inputs=[InputSpec(name='input_df', type='pd.DataFrame', description=None, default=None, optional=False)], outputs=[OutputSpec(name='Output', type='pd.DataFrame', description=None)], implementation=ContainerImplementation(container=ContainerSpec(image='tensorflow/tensorflow:1.13.2-py3', command=['sh', '-c', '(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'scikit-learn==0.19.1\\' \\'numpy==1.17.2\\' \\'pandas==0.25.1\\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\'scikit-learn==0.19.1\\' \\'numpy==1.17.2\\' \\'pandas==0.25.1\\' --user) && \"$0\" \"$@\"', 'python3', '-u', '-c', 'def one_hot_encode(input_df )  : #file_name: InputPath(\\'CSV\\')) -> OutputPath:\\n\\n#                    out_file1_name: str, \\n#                    out_file2_name: str) -> NamedTuple(\\'Outputs\\',\\n#                                                       [(\\'out_file1_name\\', OutputPath(\\'CSV\\')),\\n#                                                        (\\'out_file2_name\\', OutputPath(\\'CSV\\'))]):\\n    #out_file2_name: OutputPath(\\'CSV\\')) -> None:\\n\\n    ## Import Required Libraries\\n    import pandas as pd\\n    import matplotlib.pyplot as plt\\n    import numpy as np\\n    import sklearn\\n\\n    df_churn = input_df #pd.read_csv(file_name)  \\n\\n    binary_cols = [\\'Partner\\',\\'Dependents\\',\\'PhoneService\\',\\'PaperlessBilling\\']\\n\\n    for i in binary_cols:\\n        df_churn[i] = df_churn[i].replace({\"Yes\":1,\"No\":0})\\n\\n    #Encoding column \\'gender\\'\\n    df_churn[\\'gender\\'] = df_churn[\\'gender\\'].replace({\"Male\":1,\"Female\":0})\\n\\n    category_cols = [\\'PaymentMethod\\',\\'MultipleLines\\',\\'InternetService\\',\\'OnlineSecurity\\',\\n                   \\'OnlineBackup\\',\\'DeviceProtection\\',\\n                   \\'TechSupport\\',\\'StreamingTV\\',\\'StreamingMovies\\',\\'Contract\\']\\n\\n    for cc in category_cols:\\n        dummies = pd.get_dummies(df_churn[cc], drop_first=False)\\n        dummies = dummies.add_prefix(\"{}#\".format(cc))\\n        df_churn.drop(cc, axis=1, inplace=True)\\n        df_churn = df_churn.join(dummies)\\n\\n    df_churn[\\'Churn\\'] = df_churn[\\'Churn\\'].replace({\"Yes\":1,\"No\":0})\\n\\n    #saving files may need a PV allocation to container\\n    #output of files as Named tuple may cause problems    \\n\\n    #df_churn.to_csv(\\'Oht_enc_data.csv\\')\\n    #out_path = \"./Oht_enc_data.csv\"\\n    return df_churn #out_path\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\\'One hot encode\\', description=\\'\\')\\n_parser.add_argument(\"--input-df\", dest=\"input_df\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\\n_parsed_args = vars(_parser.parse_args())\\n_output_files = _parsed_args.pop(\"_output_paths\", [])\\n\\n_outputs = one_hot_encode(**_parsed_args)\\n\\nif not hasattr(_outputs, \\'__getitem__\\') or isinstance(_outputs, str):\\n    _outputs = [_outputs]\\n\\n_output_serializers = [\\n    str,\\n\\n]\\n\\nimport os\\nfor idx, output_file in enumerate(_output_files):\\n    try:\\n        os.makedirs(os.path.dirname(output_file))\\n    except OSError:\\n        pass\\n    with open(output_file, \\'w\\') as f:\\n        f.write(_output_serializers[idx](_outputs[idx]))\\n'], args=['--input-df', InputValuePlaceholder(input_name='input_df'), '----output-paths', OutputPathPlaceholder(output_name='Output')], env=None, file_outputs=None)), version='google.com/cloud/pipelines/component/v1')), arguments={'input_df': 'OrderedDict([(\\'Output\\', TaskOutputArgument(task_output=TaskOutputReference(output_name=\\'Output\\', task_id=None, task=TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name=\\'Read data\\', description=None, metadata=None, inputs=[InputSpec(name=\\'file_name\\', type=\\'CSV\\', description=None, default=None, optional=False)], outputs=[OutputSpec(name=\\'Output\\', type=\\'pd.DataFrame\\', description=None)], implementation=ContainerImplementation(container=ContainerSpec(image=\\'tensorflow/tensorflow:1.13.2-py3\\', command=[\\'sh\\', \\'-c\\', \\'(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\\\\\'scikit-learn==0.19.1\\\\\\' \\\\\\'numpy==1.17.2\\\\\\' \\\\\\'pandas==0.25.1\\\\\\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location \\\\\\'scikit-learn==0.19.1\\\\\\' \\\\\\'numpy==1.17.2\\\\\\' \\\\\\'pandas==0.25.1\\\\\\' --user) && \"$0\" \"$@\"\\', \\'python3\\', \\'-u\\', \\'-c\\', \\'def read_data(file_name )  :   \\\\n\\\\n    #OutputPath(\\\\\\'CSV\\\\\\'):\\\\n        # -> NamedTuple(\\\\\\'Outputs\\\\\\', [(\\\\\\'Cols_drop\\\\\\', int),(\\\\\\'Cols_retained\\\\\\', int)]):\\\\n\\\\n    ## Import Required Libraries\\\\n    import pandas as pd\\\\n    import matplotlib.pyplot as plt\\\\n    import numpy as np\\\\n    import sklearn\\\\n\\\\n    #This line may cause problems as file is on the system and not inside container\\\\n\\\\n    df_churn = pd.read_csv(file_name)\\\\n    col1 = len(df_churn.columns)\\\\n    df_churn = df_churn.drop(columns=[])\\\\n\\\\n    empty_cols=[\\\\\\'customerID\\\\\\', \\\\\\'gender\\\\\\', \\\\\\'SeniorCitizen\\\\\\', \\\\\\'Partner\\\\\\', \\\\\\'Dependents\\\\\\',\\\\n           \\\\\\'tenure\\\\\\', \\\\\\'PhoneService\\\\\\', \\\\\\'MultipleLines\\\\\\', \\\\\\'InternetService\\\\\\',\\\\n           \\\\\\'OnlineSecurity\\\\\\', \\\\\\'OnlineBackup\\\\\\', \\\\\\'DeviceProtection\\\\\\',\\\\\\'TechSupport\\\\\\',\\\\n           \\\\\\'StreamingTV\\\\\\', \\\\\\'StreamingMovies\\\\\\', \\\\\\'Contract\\\\\\', \\\\\\'PaperlessBilling\\\\\\',\\\\n           \\\\\\'PaymentMethod\\\\\\', \\\\\\'MonthlyCharges\\\\\\', \\\\\\'TotalCharges\\\\\\', \\\\\\'Churn\\\\\\']\\\\n\\\\n    for i in empty_cols:\\\\n        df_churn[i]=df_churn[i].replace(\" \",np.nan)\\\\n\\\\n    df_churn.drop(\\\\\\'customerID\\\\\\',\\\\\\'cluster number\\\\\\', axis=1, inplace=True)\\\\n    df_churn = df_churn.dropna()\\\\n\\\\n    col2 = len(df.columns)\\\\n    #df_churn.to_csv(\\\\\\'Cleaned_data.csv\\\\\\')\\\\n    #out_path = \"./Cleaned_data.csv\"\\\\n\\\\n    return df_churn\\\\n\\\\nimport argparse\\\\n_parser = argparse.ArgumentParser(prog=\\\\\\'Read data\\\\\\', description=\\\\\\'\\\\\\')\\\\n_parser.add_argument(\"--file-name\", dest=\"file_name\", type=str, required=True, default=argparse.SUPPRESS)\\\\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\\\\n_parsed_args = vars(_parser.parse_args())\\\\n_output_files = _parsed_args.pop(\"_output_paths\", [])\\\\n\\\\n_outputs = read_data(**_parsed_args)\\\\n\\\\nif not hasattr(_outputs, \\\\\\'__getitem__\\\\\\') or isinstance(_outputs, str):\\\\n    _outputs = [_outputs]\\\\n\\\\n_output_serializers = [\\\\n    str,\\\\n\\\\n]\\\\n\\\\nimport os\\\\nfor idx, output_file in enumerate(_output_files):\\\\n    try:\\\\n        os.makedirs(os.path.dirname(output_file))\\\\n    except OSError:\\\\n        pass\\\\n    with open(output_file, \\\\\\'w\\\\\\') as f:\\\\n        f.write(_output_serializers[idx](_outputs[idx]))\\\\n\\'], args=[\\'--file-name\\', InputPathPlaceholder(input_name=\\'file_name\\'), \\'----output-paths\\', OutputPathPlaceholder(output_name=\\'Output\\')], env=None, file_outputs=None)), version=\\'google.com/cloud/pipelines/component/v1\\')), arguments={\\'file_name\\': \\'https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv\\'}, is_enabled=None, execution_options=None), type=\\'pd.DataFrame\\')))])'}, is_enabled=None, execution_options=None), type='pd.DataFrame')))])\".\n",
      "  serialized_value),\n"
     ]
    }
   ],
   "source": [
    "kfp_rf_model = kfp.components.create_component_from_func(func = rf_model, \n",
    "                                                          output_component_file = './rf-model-func.yaml',\n",
    "                                                          #base_image = 'fastgenomics/sklearn',\n",
    "                                                          packages_to_install = ['scikit-learn==0.19.1','numpy==1.17.2','pandas==0.25.1','imbalanced-learn==0.6.2'])\n",
    "rf_model_task = kfp_rf_model(one_hot_encode_task.outputs, 100)     #('One_Hot_encoded_data.csv') #,'Churn_flags.csv','model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "@dsl.pipeline(name='Telco-Churn-Pipeline',description='A pipeline that processes Telco Churn dataset from Kaggle and performs ML-Predictions using Random Forest Algorithm')\n",
    "def Telco_Churn(file_name = \"https://raw.githubusercontent.com/rujual/telco_churn/master/Data.csv\", \n",
    "                n_estimators = 100):\n",
    "    read_data_task = kfp_read_data(file_name)\n",
    "    one_hot_encode_task = kfp_one_hot_encode(read_data_task.output)\n",
    "    rf_model_task = kfp_rf_model(one_hot_encode_task.output, n_estimators = 100)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruju/anaconda3/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"100\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "pipeline_func = Telco_Churn\n",
    "pipeline_filename = pipeline_func.__name__+'.pipeline.tar.gz'\n",
    "\n",
    "import kfp.compiler as comp\n",
    "comp.Compiler().compile(pipeline_func, pipeline_filename)  #, package_path='/home/My_Workplace/Telco_churn/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
